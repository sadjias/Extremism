{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import math\n",
    "#....................................\n",
    "\n",
    "#import gensim\n",
    "import texthero as hero\n",
    "#from texthero import preprocessing\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "#.....................................\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extremists = pd.read_pickle(r'all_extremists.pkl')\n",
    "df_left_extremists = pd.read_pickle(r'all_left_extremists.pkl')\n",
    "df_right_extremists = pd.read_pickle(r'all_right_extremists.pkl')\n",
    "df_nonex = pd.read_pickle(r'non_extremists.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_extremists.shape) # 851860\n",
    "print(df_left_extremists.shape) # 276585\n",
    "print(df_right_extremists.shape) # 279831\n",
    "print(df_nonex.shape) # 295444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_right_extremists['user.screen_name'].nunique()) # 100\n",
    "print(df_left_extremists['user.screen_name'].nunique()) # 100\n",
    "print(df_nonex['user.screen_name'].nunique()) # 100\n",
    "print(df_extremists['user.screen_name'].nunique()) # 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extremists # 851860 Ã— 13 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create smaller dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User features:\n",
    "- screenname, description, tweets, friends, followers, verified, listed\n",
    "\n",
    "\n",
    "\n",
    "Tweet features:\n",
    "- text, created_at, retweet count, favorite count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extreme = df_extremists[['id','label', 'user.screen_name', 'text', 'lang']] # 851860\n",
    "df_extreme = pd.DataFrame(df_extreme)\n",
    "df_extreme\n",
    "\n",
    "# ex.to_csv(\"extremist_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Check for empty rows and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extreme.loc[df_extreme['text']== '']\n",
    "df_extreme.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total: 851860 tweets\n",
    "tweets = df_extreme.groupby('label')['text'].count().to_frame(name='count')\n",
    "tweets['percentage'] = ((tweets['count'] / tweets['count'].sum()) * 100).round(1)\n",
    "print(tweets)\n",
    "\n",
    "fig = px.bar(tweets, y = \"count\", text=\"percentage\", labels=dict(label=\"Extremist groups\", count=\"\"))\n",
    "fig.update_layout(title=\"Percentage of tweets per group\", title_x= 0.5, showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# 32.5% - 34.7% - 32.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Numeric coding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_code = {\n",
    "    'LWE': 0,\n",
    "    'NE': 1,\n",
    "    'RWE': 2}\n",
    "\n",
    "df_extreme = df_extreme.replace({'label': label_code})\n",
    "df_extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_extreme = df_extreme.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme = ex[((ex['label'] == 'LWE') | (ex['label'] == 'RWE'))]\n",
    "# nonextreme = ex[ex['label'] == 'NE']\n",
    "# # 649701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(extreme.shape) # 0.625%\n",
    "# print(nonextreme.shape) # 0.375%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to clean text:\n",
    "1. Remove noise:\n",
    "    - remove URLS + HTML tags\n",
    "    - remove mentions\n",
    "    - remove hashtags\n",
    "    - remove numbers\n",
    "    - remove emojis\n",
    "    - remove punctuation\n",
    "    - remove retweets\n",
    "    - remove empty values\n",
    "\n",
    "\n",
    "2. Deep cleaning\n",
    "    - all text to lowercase\n",
    "    - remove stopwords + add own stopwords\n",
    "    - spell correction let's > let us etc.\n",
    "    - stemming\n",
    "    - lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_extreme.copy() # 851860\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Remove noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeEncodingCharacters(tweetColumn):\n",
    "    tweetColumn.replace(r'\\b[a-zA-Z]\\b', '', regex=True, inplace=True)\n",
    "\n",
    "removeEncodingCharacters(df_clean['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['text'] = df_clean['text'].str.replace(r'[\\'\\\",]*', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop_duplicates(subset='text',inplace=True)\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeRetweets(df):\n",
    "    df = df[~df['text'].str.startswith('RT')]\n",
    "    return df\n",
    "\n",
    "df_clean = removeRetweets(df_clean)\n",
    "df_clean.shape # 627429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURLs(tweetColumn):\n",
    "    tweetColumn.replace(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", regex=True, inplace=True)\n",
    "\n",
    "def removeEmojis(tweetColumn):\n",
    "    tweetColumn.replace(\"[(\\U0001F600-\\U0001F92F|\\U0001F300-\\U0001F5FF|\\U0001F680-\\U0001F6FF|\\U0001F190-\\U0001F1FF|\\U00002702-\\U000027B0|\\U0001F926-\\U0001FA9F|\\u200d|\\u2640-\\u2642|\\u2600-\\u2B55|\\u23cf|\\u23e9|\\u231a|\\ufe0f)]\", \"\", regex=True, inplace=True)\n",
    "    \n",
    "def unescapeHTML(tweetColumn):\n",
    "    return tweetColumn.apply(html.unescape)\n",
    "\n",
    "def removeMentions(tweetColumn):\n",
    "    tweetColumn.replace(\"@[A-Za-z0-9]+\", \"\", regex=True, inplace=True)\n",
    "    \n",
    "def removeNewLines(tweetColumn):\n",
    "    tweetColumn.replace(\"(\\r\\n|\\r|\\n)\", \"\", regex=True, inplace=True)\n",
    "\n",
    "def removeHashtags(tweetColumn):\n",
    "    tweetColumn.replace('([#])','', regex=True, inplace=True)\n",
    "    \n",
    "def removeNumbers(tweetColumn):\n",
    "    tweetColumn.replace(\"[0-9]\", \"\", regex=True, inplace=True)\n",
    "    \n",
    "def splitWordsInHashtag(tweetColumn):\n",
    "    tweetColumn.replace('([A-Z][a-z]+)', r' \\1', regex=True, inplace=True)  \n",
    "    \n",
    "def removeEmptyValues(df):\n",
    "    # drop empty rows  \n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    df.dropna(subset = [\"cleaned_text_punc\"], inplace=True)\n",
    "\n",
    "# combine all cleaning functions together    \n",
    "#.................................................................................    \n",
    "def noiseCleaner(df, columnToBeCleaned):\n",
    "    removeURLs(df[columnToBeCleaned])\n",
    "    removeEmojis(df[columnToBeCleaned])\n",
    "    df[columnToBeCleaned] = unescapeHTML(df[columnToBeCleaned])\n",
    "    removeMentions(df[columnToBeCleaned])\n",
    "    removeNewLines(df[columnToBeCleaned])\n",
    "    removeHashtags(df[columnToBeCleaned])\n",
    "    removeNumbers(df[columnToBeCleaned])\n",
    "    splitWordsInHashtag(df[columnToBeCleaned])\n",
    "    removeEmptyValues(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create new column with cleaned text\n",
    "#.................................................................................\n",
    "df_clean['cleaned_text_punc'] = df_clean['text']\n",
    "\n",
    "df_clean = noiseCleaner(df_clean, \"cleaned_text_punc\")\n",
    "\n",
    "df_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape # 610862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Deep cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_clean['cleaned_text_punc'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heavyCleaner(series):\n",
    "    custom_pipeline = [hero.fillna,\n",
    "                   hero.lowercase,\n",
    "                   hero.remove_whitespace,\n",
    "                   hero.remove_diacritics]\n",
    "    \n",
    "    return hero.clean(series, custom_pipeline)\n",
    "\n",
    "df_clean['cleaned_text_punc'] = heavyCleaner(df_clean['cleaned_text_punc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/\n",
    "\n",
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\", \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\", \n",
    "                     \"didn't\": \"did not\",\"doesn't\": \"does not\", \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\", \"he'd've\": \"he would have\",\"he'll\": \"he will\", \n",
    "                     \"he'll've\": \"he will have\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"I'd\": \"I would\", \n",
    "                     \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it's\": \"it is\", \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\", \n",
    "                     \"let's\": \"let us\",\"ma'am\": \"madam\", \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\", \n",
    "                     \"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n",
    "                     \"she'd\": \"she would\",\"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                     \"she'll've\": \"she will have\",\"should've\": \"should have\",\"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                     \"so've\": \"so have\",\"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\n",
    "                     \"today's\": \"today is\", \"wasn't\": \"was not\",\"we'd\": \"we would\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\n",
    "                     \"we'll've\": \"we will have\", \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\"when've\": \"when have\",\"where'd\": \"where did\", \n",
    "                     \"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                     \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\", \"w/\": \"with\",\n",
    "                     \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\n",
    "                     \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text, contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions in the reviews\n",
    "df_clean['cleaned_text_punc'] = df_clean['cleaned_text_punc'].apply(lambda x: expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['cleaned_text'] = hero.clean(df_clean['cleaned_text_punc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['cleaned_text'] = hero.clean(df_clean['cleaned_text_punc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "def removePunctuation(tweetColumn):\n",
    "    tweetColumn.replace('[^\\w\\s]',\" \", regex=True, inplace=True)\n",
    "\n",
    "removePunctuation(df_clean['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "df_clean['cleaned_text'] = df_clean['cleaned_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding stopwords\n",
    "add_words = [\"pm\", \"th\", \"im\", \"u\", \"w\", \"via\", \"sc\", \"est\", \"st\", \"dr\", \"cf\", \"iii\", \"ill\", \"en\", \"oh\", \"co\", \"rd\",\n",
    "            \"nd\", \"et\", \"rt\", \"ca\", \"al\", \"ag\", \"ye\", \"hs\", \"ky\", \"awd\", \"de\", \"ya\", \"ed\", \"ha\", \"m\", \"hi\", \"con\", \n",
    "             \"ex\", \"va\", \"que\", \"mi\", \"pr\", \"yr\", \"fo\", \"ten\", \"se\", \"nh\", \"dhs\", \"wa\", \"wi\", \"as\", \"fl\", \"wv\", \"den\"\n",
    "             \"yo\", \"wh\", \"e\", \"op\", \"mt\", \"il\", \"tx\", \"ppp\", \"da\", \"aka\", \"u\", \"ppl\", \"jr\", \"afa\", \"fr\", \"az\", \"mag\", \n",
    "             \"ou\", \"ar\", \"su\", \"per\", \"doj\", \"pt\", \"aka\", \"nc\", \"sec\", \"lee\", \"em\", \"ne\", \"pst\", \"lo\", \"ho\", \"po\", \n",
    "             \"tha\", \"dec\", \"tru\", \"ac\", \"mar\", \"mc\", \"lea\", \"dem\", \"ml\", \"utr\", \"huh\", \"pdx\", \"unm\", \"jax\", \"yup\", \n",
    "             \"yep\", \"wtf\", \"lol\", \"hey\", \"omg\", \"nah\", \"gqp\", \"yay\", \"img\", \"tbt\", \"fyi\", \"ugh\", \"duh\", \"fam\", \"yea\", \n",
    "             \"tho\", \"ily\", \"pic\", \"sum\", \"bff\", \"buh\", \"bye\", \"fav\", \"hbd\", \"nsr\", \"min\", \"ms\", \"hrs\", \"umm\", \"yum\", \n",
    "             \"cgi\", \"ai\", \"nt\", \"yas\", \"atl\", \"bhm\", \"irl\", \"fbf\", \"ter\", \"fs\", \"ish\", \"lib\", \"ley\", \"wut\", \"mlk\", \n",
    "             \"nkc\", \"nt\", \"oof\", \"pls\", \"rep\", \"rva\", \"sry\", \"tbh\", \"te\", \"ura\", \"Wan\", \"ah\", \"awe\", \"gon\", \"woo\",\n",
    "             \"hoo\", \"aww\", \"aye\", \"ayo\", \"bbm\", \"dox\", \"ftw\", \"doin\", \"ok\", \"ijs\", \"jan\", \"jeb\", \"jen\", \"ff\", \"ep\", \n",
    "             \"jus\", \"kel\", \"tec\", \"kno\", \"koo\", \"lbs\", \"luh\", \"luv\", \"mtl\", \"cdt\", \"tri\", \"tw\", \"th\", \"fai\", \"fro\", \n",
    "             \"cst\", \"fm\", \"cou\", \"br\", \"mnt\", \"imm\", \"tnm\", \"int\", \"el\", \"paso\", \"thetnm\", \"cu\", \"wo\", \"si\", \"ope\", \n",
    "             \"wot\", \"yes\", \"xe\", \"xf\", \"xa\", \"xs\", \"xc\", \"xd\", \"xef\", \"xb\", \"xm\", \"xre\", \"xt\", \"etc\", \"xaa\", \"xba\",\n",
    "            \"xcp\", \"iuic\", \"pttw\", \"tix\", \"uno\", \"xbn\", \"por\", \"los\", \"la\", \"del\", \"xbos\", \"kjv\", \"tldr\", \"xadoes\", \n",
    "             \"mr\", \"acab\", \"dj\", \"go\", \"live\", \"life\", \"like\", \"see\", \"want\", \"let\", \"may\", \"way\", \"say\", \"xve\", \n",
    "             \"xbe\", \"xll\", \"ni\", \"xbc\", \"xbf\"]\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_added = stop_words.union(add_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['cleaned_text'] = df_clean['cleaned_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_added))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove single letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing single stand alone letters\n",
    "df_clean['cleaned_text'].replace(r\"\\b[a-zA-Z]\\b\", \"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(df_clean['cleaned_text']).split()).value_counts()\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hashtag splitter\n",
    "##.................................................................................................\n",
    "# import nltk\n",
    "# from nltk.corpus import words, brown\n",
    "\n",
    "# word_dictionary = list(set(words.words()))\n",
    "\n",
    "# for alphabet in \"bcdefghjklmnopqrstuvwxyz\":\n",
    "#     word_dictionary.remove(alphabet)\n",
    "\n",
    "# def split_hashtag_to_words_all_possibilities(hashtag):\n",
    "#     all_possibilities = []\n",
    "    \n",
    "#     split_posibility = [hashtag[:i] in word_dictionary for i in reversed(range(len(hashtag)+1))]\n",
    "#     possible_split_positions = [i for i, x in enumerate(split_posibility) if x == True]\n",
    "    \n",
    "#     for split_pos in possible_split_positions:\n",
    "#         split_words = []\n",
    "#         word_1, word_2 = hashtag[:len(hashtag)-split_pos], hashtag[len(hashtag)-split_pos:]\n",
    "        \n",
    "#         if word_2 in word_dictionary:\n",
    "#             split_words.append(word_1)\n",
    "#             split_words.append(word_2)\n",
    "#             all_possibilities.append(split_words)\n",
    "            \n",
    "#             another_round = split_hashtag_to_words_all_possibilities(word_2)\n",
    "            \n",
    "#             if len(another_round) > 0:\n",
    "#                 all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n",
    "#         else:\n",
    "#             another_round = split_hashtag_to_words_all_possibilities(word_2)\n",
    "            \n",
    "#             if len(another_round) > 0:\n",
    "#                 all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in zip([word_1]*len(another_round), another_round)]\n",
    "                \n",
    "#     return all_possibilities\n",
    "\n",
    "# split_hashtag_to_words_all_possibilities(\"sheiscoming\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[df_clean['cleaned_text'].str.strip().astype(bool)]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[ (df_clean['lang'] == 'ca') | (df_clean['lang'] == 'cy') | \n",
    "         (df_clean['lang'] == 'da') | (df_clean['lang'] == 'de') | \n",
    "         (df_clean['lang'] == 'en') | (df_clean['lang'] == 'es') |\n",
    "         (df_clean['lang'] == 'et') | (df_clean['lang'] == 'eu') |\n",
    "         (df_clean['lang'] == 'in') | (df_clean['lang'] == 'it') |\n",
    "         (df_clean['lang'] == 'lt') | (df_clean['lang'] == 'nl') |\n",
    "         (df_clean['lang'] == 'no') | (df_clean['lang'] == 'pl') |\n",
    "         (df_clean['lang'] == 'pt') | (df_clean['lang'] == 'ro') |\n",
    "         (df_clean['lang'] == 'tl') | (df_clean['lang'] == 'und')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset index of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.reset_index(drop=True)\n",
    "df_clean.shape # 590216"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(df_clean['cleaned_text']).split()).value_counts()\n",
    "freq[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "#lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Loading model\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n",
    "\n",
    "# def lemmatization(text):\n",
    "#     return [lemmatizer.lemmatize(x) for x in tokenizer.tokenize(text)]\n",
    "    \n",
    "df_clean['lemmatized_text'] = df_clean['cleaned_text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop == False)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove single letters after lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['cleaned_text'] = df_clean['lemmatized_text']\n",
    "df_clean['cleaned_text'].replace(r\"\\b[a-zA-Z]\\b\", \"\", regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(df_clean['cleaned_text']).split()).value_counts()\n",
    "freq[60:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all rows where avg_word_len < 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/(len(words)+0.000001))\n",
    "\n",
    "df_clean['avg_word_len'] = df_clean['cleaned_text'].apply(lambda x: avg_word(x)).round(2)\n",
    "df_clean.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[df_clean['cleaned_text'].str.strip().astype(bool)] # 589268\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows with single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean[df_clean['avg_word_len'] >= 3]\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heavyCleaner(df_clean['cleaned_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_clean.drop(['lemmatized_text', 'lang', 'avg_word_len'], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save cleaned dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset index of cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.reset_index(drop=True) # 581470 rows\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_pickle(\"cleaned_tweets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_pickle(\"cleaned_tweets.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total: 581470 tweets\n",
    "tweets = df_final.groupby('label')['cleaned_text'].count().to_frame(name='count')\n",
    "tweets['percentage'] = ((tweets['count'] / tweets['count'].sum()) * 100).round(1)\n",
    "print(tweets)\n",
    "\n",
    "fig = px.bar(tweets, y = \"count\", text=\"percentage\", labels=dict(label=\"Extremist groups\", count=\"\"))\n",
    "fig.update_layout(title=\"Percentage of tweets per group\", title_x= 0.5, showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# 24%, 40.3%, 35.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.groupby(['label'])['user.screen_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
