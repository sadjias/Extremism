{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af66a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.......... for data .................\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "#.......... for plotting ..............\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import plotly.tools as tls\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75b9f5",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('balanced_tweets.pkl')\n",
    "df # 418938"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c97e21",
   "metadata": {},
   "source": [
    "# 1. Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc823d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca36afc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature(df):\n",
    "    df['word_count'] = df['cleaned_text'].apply(lambda x : len(x.split()))\n",
    "    df['char_count'] = df['cleaned_text'].apply(lambda x : len(x.replace(\" \",\"\")))\n",
    "    df['word_density'] = df['word_count'] / (df['char_count'] + 1)\n",
    "    df['punc_count'] = df['cleaned_text_punc'].apply(lambda x : len([a for a in str(x) if a in string.punctuation]))\n",
    "    df['tweet_length'] = df['cleaned_text'].apply(len)\n",
    "    df['upper_count'] = df['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "    df['ratio_upper_lower'] = (df['upper_count'] / df['tweet_length']).round(2)\n",
    "    df['hashtag_count'] = df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "    df['num_unique_words'] = df['cleaned_text'].apply(lambda x: len(set(w for w in x.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['word_count']\n",
    "    df[\"word_unique_percent\"] =  df[\"num_unique_words\"]*100/df['word_count'].round(2)\n",
    "    return df\n",
    "\n",
    "feature(df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39436643",
   "metadata": {},
   "source": [
    "# 2. Part-of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9406e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used from: https://towardsdatascience.com/how-i-improved-my-text-classification-model-with-feature-engineering-98fbe6c13ef3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00745ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lwe = df_features[df_features['label'] == 0]\n",
    "ne = df_features[df_features['label'] == 1]\n",
    "rwe = df_features[df_features['label'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lwe = \" \".join(lwe.cleaned_text)\n",
    "text_ne = \" \".join(ne.cleaned_text)\n",
    "text_rwe = \" \".join(rwe.cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6adfd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_lwe.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_rwe.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6748440",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_lwe.split()) - len(text_rwe.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1ac691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    \"\"\"\n",
    "    split the document into sentences and tokenize each sentence\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self,text):\n",
    "        # split into single sentence\n",
    "        sentences = self.splitter.tokenize(text)\n",
    "        # tokenization in each sentences\n",
    "        tokens = [self.tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "class LemmatizationWithPOSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def get_wordnet_pos(self,treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "    def pos_tag(self,tokens):\n",
    "        # find the pos tagging for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....\n",
    "        pos_tokens = [nltk.pos_tag(token) for token in tokens]\n",
    "    \n",
    "        # lemmatization using pos tagg   \n",
    "        #  [('What', 'What', ['WP']), ('can', 'can', ['MD']) --> WORD/LEMMA/POSTAG\n",
    "        pos_tokens = [[(word, lemmatizer.lemmatize(word,self.get_wordnet_pos(pos_tag)),\n",
    "                         pos_tag) for (word, pos_tag) in pos] for pos in pos_tokens]\n",
    "        return pos_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760aff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "splitter = Splitter()\n",
    "lemmatization_using_pos_tagger = LemmatizationWithPOSTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39fcc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = splitter.split(text_lwe)\n",
    "lwe_list = lemmatization_using_pos_tagger.pos_tag(tokens)\n",
    "print(lwe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd438a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = splitter.split(text_ne)\n",
    "ne_list = lemmatization_using_pos_tagger.pos_tag(tokens)\n",
    "#print(ne_list[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa784ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = splitter.split(text_rwe)\n",
    "rwe_list = lemmatization_using_pos_tagger.pos_tag(tokens)\n",
    "#print(rwe_list[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31dd385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_features(list_) :\n",
    "    list_weight = len(' '.join(str(v[0][0]) for v in list_))\n",
    "    POS_DICT = {}\n",
    "    POS_DICT['Nouns'] = sum([sum(1 for words in sentence if words[2] == 'NN' or words[2] == 'NNS' or words[2] == 'NNP' or words[2] == 'NNP') \n",
    "                                   for sentence in list_])\n",
    "    POS_DICT['Past'] = sum([sum(1 for words in sentence if words[2] == 'VBD') \n",
    "                                   for sentence in list_])\n",
    "    POS_DICT['Superlative Adverb'] = sum([sum(1 for words in sentence if words[2] == 'RBS') # fastest\n",
    "                                   for sentence in list_])\n",
    "    POS_DICT['Adjectives'] = sum([sum(1 for words in sentence if words[2] == 'JJ' or words[2] == 'JJR' or words[2] == 'JJS') \n",
    "                                   for sentence in list_])\n",
    "    POS_DICT['Possesive pronoun'] = sum([sum(1 for words in sentence if words[2] == 'WP$') \n",
    "                                   for sentence in list_])\n",
    "    POS_DICT['Personal Pronoun'] = sum([sum(1 for words in sentence if words[2] == 'PRP') \n",
    "                                   for sentence in list_])\n",
    "    POS_DICT['possesive pronoun'] = sum([sum(1 for words in sentence if words[2] == 'PRP$') \n",
    "                                   for sentence in list_])\n",
    "    POS_DICT['superlative adj'] = sum([sum(1 for words in sentence if words[2] == 'JJS') # biggest\n",
    "                                   for sentence in list_])\n",
    "    POS_DICT['Verb'] = sum([sum(1 for words in sentence if words[2] == 'VB' or words[2] == \"VBN\" or words[2] == \"VB\" \\\n",
    "                                      or words[2] == \"VBG\" or words[2] == \"VBP\" or words[2] == \"VBZ\" or words[2] == \"VBD\") \n",
    "                                   for sentence in list_])\n",
    "    POS_DICT[\"singular noun\"] = sum([sum(1 for words in sentence if words[2] == 'NN') # one person\n",
    "                                     for sentence in list_])/ list_weight\n",
    "    POS_DICT[\"plural names\"] = sum([sum(1 for words in sentence if words[2] == 'NNS') \n",
    "                                   for sentence in list_])\n",
    "    \n",
    "    return pd.DataFrame(POS_DICT, index = range(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f25630",
   "metadata": {},
   "source": [
    "## List of POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lwe_df = pos_tag_features(lwe_list)\n",
    "lwe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rwe_df = pos_tag_features(rwe_list)\n",
    "rwe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f479d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_form = [x for x in lwe_df.columns]\n",
    "tagged_label = [str(x) for x in tagged_form]\n",
    "tagged_dict = dict(zip(tagged_label, tagged_form))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22687a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tagged_dict.items():\n",
    "    print(lwe_df[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tls.make_subplots(rows=1, cols=2, specs = [[{}, {}]], subplot_titles=(\"Verbal distribution of tweets from LWE\", \"Verbal distribution of tweets from RWE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b84e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tagged_dict.items():\n",
    "    tag_trace = go.Bar(x=lwe_df[v], name = str(k), text=k, textposition = 'auto', \n",
    "                       marker=dict( color='rgb(221,160,221)',line=dict(color='rgb(8,48,107)',width=1.5),),\n",
    "                       opacity=0.6, showlegend=False)\n",
    "    fig.append_trace(tag_trace, 1, 1)\n",
    "\n",
    "\n",
    "for k,v in tagged_dict.items():\n",
    "    tag_trace = go.Bar(x=rwe_df[v], name = str(k), text=k, textposition = 'auto', \n",
    "                       marker=dict( color='rgb(239, 243, 198)',line=dict(color='rgb(8,48,107)',width=1.5),),\n",
    "                       opacity=0.6, showlegend=False)\n",
    "    fig.append_trace(tag_trace, 1, 2)\n",
    "    \n",
    "\n",
    "\n",
    "fig.update_layout({\n",
    "'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "}, title_x=0.5)\n",
    "\n",
    "fig['layout'].update(height=800, width=1000,\n",
    ")  \n",
    "\n",
    "fig.update_xaxes(title_text=\"Part of Speech Count\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Part of Speech Count\", row=1, col=2)\n",
    "# fig.update_xaxes(\n",
    "#     ticktext=[\"200k\", \"400k\"],\n",
    "#     tickvals=[0.2,],\n",
    "# )\n",
    "\n",
    "# fig.update_layout(\n",
    "#     xaxis = dict(\n",
    "#         tickmode = 'array',\n",
    "#         tickvals = [0.2, 0.4, 0.6, 0.8],\n",
    "#         ticktext = [\"200k\", \"400k\", \"600k\", \"800k\"]\n",
    "#     )\n",
    "# )\n",
    "\n",
    "fig.update_traces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(text):\n",
    "    pos=nltk.pos_tag(word_tokenize(text))\n",
    "    pos=list(map(list,zip(*pos)))[1]\n",
    "    return pos\n",
    "\n",
    "tags=lwe['cleaned_text'].apply(lambda x : pos(x))\n",
    "tags=[x for l in tags for x in l]\n",
    "counter=Counter(tags)\n",
    "\n",
    "x,y=list(map(list,zip(*counter.most_common(7))))\n",
    "\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b263c",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a753b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['label'] = df_features.label.replace(to_replace=[0, 1, 2], value= ['LWE', 'NE', 'RWE'])\n",
    "df_features['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804e0e7",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b835abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average length of a tweet per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0772c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3efaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_features.groupby('label')['char_count'].mean().to_frame(name=\"mean\").round(2)\n",
    "\n",
    "fig = px.bar(tweets, y = \"mean\", text=\"mean\", labels=dict(label=\"Extremist groups\", count=\"\"))\n",
    "fig.update_layout(title=\"Average number of characters per extremist group\", title_x= 0.5, showlegend=False)\n",
    "\n",
    "fig.update_layout({\n",
    "'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "}, title_x=0.5,\n",
    "yaxis_title = \"Number of Characters\")\n",
    "\n",
    "fig.update_traces(marker_color='#1f77b4')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average word count of a tweet with stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['word_count_stop'] = df_features['cleaned_text_punc'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "tweets = df_features.groupby(['label'])['word_count_stop'].sum().to_frame(name=\"sum\")\n",
    "\n",
    "\n",
    "fig = px.bar(tweets, y = \"sum\", text=\"sum\", labels=dict(label=\"Extremist groups\", count=\"\"))\n",
    "fig.update_layout(title=\"Number of words per extremist group\", title_x= 0.5, showlegend=False)\n",
    "\n",
    "fig.update_layout({\n",
    "'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "}, title_x=0.5,\n",
    "yaxis_title = \"Number of Words\")\n",
    "\n",
    "fig.update_traces(marker_color='#1f77b4')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97817b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average word count of a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e213a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.groupby(['label'])['word_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of uppercase words compared to total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf787d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df_features.groupby(['label'])['upper_count'].sum().to_frame(name=\"sum\")\n",
    "fig = px.bar(tweets, y = \"sum\", text=\"sum\", labels=dict(label=\"Extremist groups\", count=\"\"))\n",
    "fig.update_layout(title=\"Number of uppercase words per extremist group\", title_x= 0.5, showlegend=False)\n",
    "\n",
    "fig.update_layout({\n",
    "'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "}, title_x=0.5,\n",
    "yaxis_title = \"Number of Uppercase Words\")\n",
    "\n",
    "fig.update_traces(marker_color='#1f77b4')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_upper = (df_features.groupby(['label'])['upper_count'].sum() / df_features.groupby(['label'])['word_count'].sum()).reset_index(name='count')\n",
    "ratio_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average number of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1093cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features['stopword_count'] = df_features['word_count_stop'] - df_features['word_count']\n",
    "tweets = df_features.groupby(['label'])['stopword_count'].sum().to_frame(name=\"sum\")\n",
    "\n",
    "fig = px.bar(tweets, y = \"sum\", text=\"sum\", labels=dict(label=\"Extremist groups\", count=\"\"))\n",
    "fig.update_layout(title=\"Number of stopwords per extremist group\", title_x= 0.5, showlegend=False)\n",
    "\n",
    "fig.update_layout({\n",
    "'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n",
    "}, title_x=0.5,\n",
    "yaxis_title = \"Number of Stopwords\")\n",
    "\n",
    "fig.update_traces(marker_color='#1f77b4')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe790f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top 20 Words in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73261be",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "new = df_features['cleaned_text'].str.split()\n",
    "new = new.values.tolist()\n",
    "corpus = [word for i in new for word in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "counter = Counter(corpus)\n",
    "most = counter.most_common()\n",
    "\n",
    "x, y = [], []\n",
    "for word,count in most[:20]:\n",
    "    if (word not in stop):\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278611d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Worldcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e50031",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "for val in df_features['cleaned_text']:\n",
    "      \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    "  \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "      \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "      \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    "  \n",
    "# plot the WordCloud image                       \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.title('Top 100 Most Common Words', fontsize=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bad35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribution of words per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a6ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lwe = df_features.copy()\n",
    "df_lwe = df_lwe[df_lwe['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ne = df_features.copy()\n",
    "df_ne = df_ne[df_ne['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rwe = df_features.copy()\n",
    "df_rwe = df_rwe[df_rwe['label'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a095dba",
   "metadata": {},
   "source": [
    "## Unigram Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd411ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigrams distribution\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54613a",
   "metadata": {},
   "source": [
    "#### 1. LWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f7bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lwe = df_features.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lwe = df_lwe[df_lwe['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33160d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(df_lwe['cleaned_text'], 25)\n",
    "# for word, freq in common_words:\n",
    "#     print(word, freq)\n",
    "\n",
    "df2 = pd.DataFrame(common_words, columns = ['cleaned_text' , 'count'])\n",
    "df2.groupby('cleaned_text').sum()['count'].sort_values().iplot(\n",
    "    kind='bar', xTitle='Count', yTitle='Words', linecolor='black', title='Top 25 unigrams for LWE', orientation='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17957b",
   "metadata": {},
   "source": [
    "#### 2. NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5fb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ne = df_features.copy()\n",
    "df_ne = df_ne[df_ne['label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0052013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(df_ne['cleaned_text'], 25)\n",
    "\n",
    "df3 = pd.DataFrame(common_words, columns = ['cleaned_text' , 'count'])\n",
    "df3.groupby('cleaned_text').sum()['count'].sort_values().iplot(\n",
    "    kind='bar', xTitle='Count', yTitle='Words', linecolor='black', title='Top 25 unigrams for NE', orientation='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d4544",
   "metadata": {},
   "source": [
    "#### 3. RWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eec2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rwe = df_features.copy()\n",
    "df_rwe = df_rwe[df_rwe['label'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca73fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(df_rwe['cleaned_text'], 25)\n",
    "\n",
    "df4 = pd.DataFrame(common_words, columns = ['cleaned_text' , 'count'])\n",
    "df4.groupby('cleaned_text').sum()['count'].sort_values().iplot(\n",
    "    kind='bar', xTitle='Count', yTitle='Words', linecolor='black', title='Top 25 unigrams for RWE', orientation='h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0182f95",
   "metadata": {},
   "source": [
    "### Bigrams distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams distribution\n",
    "\n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53940c5b",
   "metadata": {},
   "source": [
    "#### 1. LWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab1cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_bigram(df_lwe['cleaned_text'], 25)\n",
    "# for word, freq in common_words:\n",
    "#     print(word, freq)\n",
    "\n",
    "df5 = pd.DataFrame(common_words, columns = ['cleaned_text', 'count'])\n",
    "df5.groupby(['cleaned_text']).sum()['count'].sort_values().iplot(\n",
    "    kind='bar', xTitle='Count', yTitle='Words', linecolor='black', title='Top 25 bigrams for LWE', orientation='h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37dadf4",
   "metadata": {},
   "source": [
    "#### 2. NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_bigram(df_ne['cleaned_text'], 25)\n",
    "\n",
    "df6 = pd.DataFrame(common_words, columns = ['cleaned_text', 'count'])\n",
    "df6.groupby(['cleaned_text']).sum()['count'].sort_values().iplot(\n",
    "    kind='bar', xTitle='Count', yTitle='Words', linecolor='black', title='Top 25 bigrams for NE', orientation='h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ef617",
   "metadata": {},
   "source": [
    "#### 3. RWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038782b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LWE\n",
    "common_words = get_top_n_bigram(df_rwe['cleaned_text'], 25)\n",
    "# for word, freq in common_words:\n",
    "#     print(word, freq)\n",
    "\n",
    "df7 = pd.DataFrame(common_words, columns = ['cleaned_text', 'count'])\n",
    "df7.groupby(['cleaned_text']).sum()['count'].sort_values().iplot(\n",
    "    kind='bar', xTitle='Count', yTitle='Words', linecolor='black', title='Top 25 bigrams for RWE', orientation='h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d5158",
   "metadata": {},
   "source": [
    "### Trigrams distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_trigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17336c",
   "metadata": {},
   "source": [
    "#### 1. LWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc187c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_trigram(df_lwe['cleaned_text'], 25)\n",
    "# for word, freq in common_words:\n",
    "#     print(word, freq)\n",
    "\n",
    "df8 = pd.DataFrame(common_words, columns = ['cleaned_text' , 'count'])\n",
    "df8.groupby('cleaned_text').sum()['count'].sort_values().iplot(\n",
    "    kind='bar', xTitle='Count', yTitle='Words', linecolor='black', title='Top 25 trigrams for LWE', orientation='h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3be7c0",
   "metadata": {},
   "source": [
    "#### 2. NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_trigram(df_ne['cleaned_text'], 25)\n",
    "# for word, freq in common_words:\n",
    "#     print(word, freq)\n",
    "\n",
    "df8 = pd.DataFrame(common_words, columns = ['cleaned_text' , 'count'])\n",
    "df8.groupby('cleaned_text').sum()['count'].sort_values().iplot(\n",
    "    kind='bar', xTitle='Count', yTitle='Words', linecolor='black', title='Top 25 trigrams for NE', orientation='h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e597e3",
   "metadata": {},
   "source": [
    "#### 3. RWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fe5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_trigram(df_rwe['cleaned_text'], 25)\n",
    "# for word, freq in common_words:\n",
    "#     print(word, freq)\n",
    "\n",
    "df8 = pd.DataFrame(common_words, columns = ['cleaned_text' , 'count'])\n",
    "df8.groupby('cleaned_text').sum()['count'].sort_values().iplot(\n",
    "    kind='bar', xTitle='Count', yTitle='Words', linecolor='black', title='Top 25 trigrams for RWE', orientation='h')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bad6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
